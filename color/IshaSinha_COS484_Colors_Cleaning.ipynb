{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following code was run numerous times for 'omcs_dataset.txt' and 'concept_net.txt' in order to automate portions of the data cleaning.\n",
        "\n",
        "A large portion of it could not be done in this manner, and had to be done manually, taking several hours, starting out with ~15,000 examples in the colors dataset. I eventually whittled it down to about ~8,000 examples total, however this proved to be too large a set of examples to manually vet given the time constraint. After working on it for ~15 hours, I decided to randomly select 3,000 sentences from the 8,000 (mostly vetted) samples to get a good sense of the common sense examples for colour.\n",
        "\n",
        "Since we intended to replicate the NumerSense paper, ~3,000 examples are selected randomly from the total compiled examples."
      ],
      "metadata": {
        "id": "zDR2W-r4XhKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Cleaning of Dataset:"
      ],
      "metadata": {
        "id": "Zqsnm3KhrARt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"\" # The file name was different each time as this was done iteratively. Check Github for all stages of processing."
      ],
      "metadata": {
        "id": "qaW1JfA2rHpE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename) as f:\n",
        "  corpus = f.read()\n",
        "samples = corpus.split(\"\\n\")"
      ],
      "metadata": {
        "id": "RlD8qS-Arb8O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_samps = []"
      ],
      "metadata": {
        "id": "ENTr9BMzxhTJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We found that examples that contained these words/phrases were not commonsense.\n",
        "buzzwords = ['description', 'understand', 'situation',\n",
        "             'things that are often found together', 'question', 'translation',\n",
        "             'synonym', 'another way', 'common name', 'type of', 'statement',\n",
        "             'story', 'picture', 'photo', 'something you might do', 'size',\n",
        "             'something that might happen', 'event', 'topic', 'something you find',\n",
        "             'this is']\n",
        "for sample in samples:\n",
        "  should_add = True\n",
        "  # Upon observation, sentences that are longer than 12 words are usually NOT\n",
        "  # common sense.\n",
        "  if len(sample.split(' ')) > 12:\n",
        "    should_add = False\n",
        "  for word in buzzwords:\n",
        "    if word in sample:\n",
        "      should_add = False\n",
        "  if should_add:\n",
        "    clean_samps.append(sample)"
      ],
      "metadata": {
        "id": "BOuvflCmvzG1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('omcs_partially_clean_5.txt', 'a') as f:\n",
        "  for samp in clean_samps:\n",
        "    f.write(samp+'\\n')"
      ],
      "metadata": {
        "id": "kWpTztEAxwhD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting 3000 examples:"
      ],
      "metadata": {
        "id": "Hizw0fHtq5x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "qVwe8y9Pr0Qv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k4ESf7cjvBOq"
      },
      "outputs": [],
      "source": [
        "with open('/content/color_7000.txt') as f:\n",
        "  corpus = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = corpus.split(\"\\n\")"
      ],
      "metadata": {
        "id": "7QhlkwqdvIe9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('number of samples after cleaning:', len(samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxDKrZ5bveFj",
        "outputId": "f0e50576-f71d-4fa0-dd1d-a5cdeb578dee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of samples after cleaning: 7926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomised_data = random.sample(samples, 3000)\n",
        "\n",
        "with open('colors_3000.txt', 'a') as f:\n",
        "  for samp in randomised_data:\n",
        "    f.write(samp+'\\n')"
      ],
      "metadata": {
        "id": "L6_gtG43rqhU"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}